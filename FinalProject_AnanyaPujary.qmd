---
title: "Automating Fact-Checking using Machine Learning Models with the FEVER Dataset"
author: "Ananya Pujary"
format: pdf
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(naniar)
library(ggplot2)
library(tidytext)
library(lsr)
library(ggmosaic)
library(rlang)
library(data.table)
library(jsonlite)
library(syuzhet)
library(knitr)
library(quanteda)
library(caret)
library(Matrix)
```

## Motivation

Fact-checking has become increasingly common in recent years because of the large amounts of misinformation now available. It can be defined as the "process of verifying facts in natural text against a database of facts" ("Fact Verification", n.d.). Previous studies have demonstrated that people tend to trust fake news relatively easily than truthful news if it confirms pre-existing beliefs, they are constantly exposed to it, or if they are under peer pressure to conform to widespread beliefs (El Houfi, 2022). For this reason, fact-checking is essential in order to "maintain a truthful digital environment" (El Houfi, 2022).

Fake news can be addressed in three ways: educating the public, manual fact-checking, and automatic classification (Taboada, 2021). The first two solutions, while effective, are not enough considering the speed at which misinformation travels. On the other hand, automating the process of fact-checking using machine learning and natural language processing techniques can assist journalists in efficiently verifying the authenticity of statements.

Automated Fact-Checking (AFC) systems are "a type of artificial intelligence that find factual claims, verify them, and correct them in real time" (Attal, 2020), and were preceded by human annotators who are part of independent fact-checking websites. They are employed by several online platforms nowadays and have three main steps: 1) Identification of 'checkable claims' from textual and verbal content using machine learning and natural language processing techniques, 2) Verification of claims by comparison to previously fact-checked claims, external libraries, or official databases; while some AFC systems label claims as true/false, others follow a rating scale indicating truthfulness, 3) Correction of untrue claims by bots or human fact-checkers (Attal, 2020).

These systems have a quick content processing time and are able to assess simple sentences with good accuracy (Attal, 2020). At the same time, they still rely on "access to human-compiled reliable sources of data and information" (Attal, 2020), and NLP and AI software have not advanced far enough to assess all claims with complete accuracy. Additionally, claim verification often depends on understanding their context, and several claims don't tend to fall within a true/false binary. Thus, fact-checking does still require a good deal of human intervention to effectively prevent the spread of misinformation.

In the fact-checking process, the stylistic and linguistic elements of a text are typically considered. Fake news tends to "contain more adverbs, more negative words, and more words related to sex, death, and anxiety" (Taboada, 2021). They use "they" pronouns more frequently, while authentic news uses the "I" pronoun most frequently (Taboada, 2021). Factual claims also tend to use more punctuation and apostrophes.

Text classification works best when labelled, large datasets are available for the algorithm to train on (Taboada, 2021). Hence, for this project, I've chosen to work with the publicly available FEVER (Fact Extraction and VERification) dataset. It includes "185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from" (Thorne et al., 2018). According to evidence collected by annotators, these claims are classified either as supported, refuted, or not enough information was available to make a decision. This project will aim to build models that try to predict whether a claim can be supported, refuted, or there is insufficient information to determine this based on existing and new predictors in the dataset.

## Exploratory Data Analysis

Importing the FEVER dataset and converting it from a .jsonl format:

```{r}
#| echo: false
con <- file("/Users/ananyapujary/Desktop/DACSS/DACSS695M/695m_final/train.jsonl","r")
fever <- jsonlite::stream_in(con, verbose = FALSE)

```

```{r}
print(summarytools::dfSummary(fever, varnumbers = FALSE, plain.ascii = FALSE, graph.magnif = 0.50, style = "grid", valid.col = FALSE), 
      method = 'render', table.classes = 'table-condensed')

```

From the above data summary, we see that the `train` dataset has 5 columns and 145449 rows. The columns are `id` (unique values for each row), `verifiable` (indicating whether the claim is verifiable or not), `label` (whether the evidence found "supports", "refutes", or is "not enough info" for the claim to be verified), `claim` (one sentence extracted from Wikipedia that has been altered), and `evidence` (a list type variable containing alphanumeric values indicating the source of evidence gathered from Wikipedia for that claim).

There are no missing values in this dataset, as we can see from the visualization below, so there's no need to impute values.

```{r}
vis_miss(fever, warn_large_data = FALSE) + theme(axis.text.x = element_text(angle=80))
```

The distribution of the target variable `label` is as follows:

```{r}
label_bar <- fever %>%
  ggplot(aes(x=label, fill = label)) + geom_bar(aes(y = (..count..)/sum(..count..))) + scale_y_continuous(labels=scales::percent) + labs(title = "Distribution of `label`") + xlab("Label") + ylab("Percentage") + theme_minimal()
label_bar + scale_fill_brewer(palette = "PuOr", guide = "none")
```

Here, "SUPPORTS" is the most frequently occurring category, followed by "NOT ENOUGH INFO", then "REFUTES".

Also checking the distribution of the `label` column:

```{r}
ver_bar <- fever %>%
  ggplot(aes(x=verifiable, fill = verifiable)) + geom_bar(aes(y = (..count..)/sum(..count..))) + scale_y_continuous(labels=scales::percent) + labs(title = "Distribution of `verifiable`") + xlab("Verifiable") + ylab("Percentage") + theme_minimal()
ver_bar + scale_fill_brewer(palette = "PuOr", guide = "none")
```

Clearly, the classes are imbalanced, with "VERIFIABLE" being more frequent than "NOT VERIFIABLE" claims.

Plotting the relationship between `label` and `verifiable`:

```{r}
lab_ver <- ggplot(data = fever) +
  geom_mosaic(aes(x=product(label, verifiable), fill = label)) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(),axis.ticks.x = element_blank(),panel.grid = element_blank()) + labs(title='Relationship between `label` and `verifiable`')
lab_ver + scale_fill_brewer(palette = "PuOr")
```

From this mosaic plot, we can observe that a claim categorized as having "NOT ENOUGH INFO" means that it is also "NOT VERIFIABLE", while a "VERIFIABLE" claim can be categorized as either "SUPPORTS" or "REFUTES".

Visualizing the frequency of words in the `claim` column:

```{r}
claim_words <- fever %>% 
                  unnest_tokens(output = word, input = claim) 
claim_words <- claim_words %>%
                   anti_join(stop_words) # removing stop words
claim_wordcounts <- claim_words %>% count(word, sort = TRUE)

claim_count <- claim_wordcounts %>% 
  filter(n > 2000) %>% 
  mutate(word = reorder(word, n)) %>% 
    ggplot(aes(x = word, y= n, fill = n)) + 
    geom_col(width = 1) +
    coord_flip() +
    labs(x = "Word", y = "Count ", title = "Frequency of terms in `claim`") +
    geom_text(aes(label = n), hjust = 1.2, colour = "white", fontface = "bold") + theme_minimal()
claim_count + scale_fill_gradient(low="#b2abd2", high="#fdb863", guide = "none")
```

The term "film" is the most frequently occurring by a long shot, followed by "american" and "born". From the terms identified above, it seems that many of the claims in this dataset is related to the film industry, most likely Hollywood-based.

Looking for punctuation in the `claim` column, since exclamation marks, ellipses, question marks, and other less formal punctuation marks can be signs of a less factually accurate sentence:

```{r}
# removing commonly used formal punctuation [.(),:'–"-@] and detecting the remaining punctuation
pattern <- "[^\\.\\(\\)\\-\\–\\,\\:\\'\\\"\\{\\}\\w\\s]"

# finding matches and extract punctuation marks
punctuation_marks <- str_extract_all(fever$claim, pattern) %>% unlist()
table(punctuation_marks)

# creating a new vector 'punctuations' with all of the punctuation marks that were detected
punctuations <- c("!", "?","$","\\","\\/","\\@","`","#","&","%","+","=")
filtered_data <- fever %>%
  select(claim,verifiable,label) %>%
  filter(stringr::str_detect(claim, paste0("[", paste(punctuations, collapse = ""), "]")))

# checking the distribution of `label` and `verifiable` in this filtered dataset
table(filtered_data$label)
table(filtered_data$verifiable)
```

There are no significant changes in the distribution of these variables in the filtered dataset when compared to the original one, so punctuation marks may not be a strong predictor of false claims in this dataset.

I'll be adding new columns based on the ones already present in the dataset. First, I'm creating a new column `claim_nchar` that provides information on the number of characters in `claim`:

```{r}
fever$claim_nchar <- nchar(fever$claim)
```

Also, I'm making a new column indicating whether there are any missing values in the `evidence` list column or not:

```{r}
# checking for missing values and creating a new column
fever <- fever %>%
  mutate(missing_evidence = evidence %>%
           map_lgl(~ any(.x == "NA")))

# replacing NA values in the new column with 'TRUE'
fever$missing_evidence <- fever$missing_evidence %>%
  replace_na(TRUE)

# sanity check
table(fever$missing_evidence)

```

Negatively worded sentences tend to be less factually accurate, so I'm making a column that indicates whether the sentiment expressed in each claim is positive or negative.

```{r}
# extracting sentiments from `claim` using the Bing lexicon that categorizes words as either positive or negative
sentiment <- get_sentiment(fever$claim, method = "bing")

#combining the sentiment score data frame and the original data frame
fever <- cbind(fever, sentiment)

# making a new column with "Positive" value for when sentiment values are from 0 and above and "Negative" when the sentiment value is below zero 
fever <- fever %>%
  mutate(sentiment_category = ifelse(sentiment >= 0, "Positive", "Negative")) %>%
  select(-sentiment) # dropping the `sentiment` column

# sanity check
table(fever$sentiment_category)
```

Reordering the columns so that the target variable is positioned last in the data frame:

```{r}
fever <- fever %>%
  relocate(label, .after = sentiment_category)
```

Now, the dataset has 8 columns and 145449 rows. `id` and `claim_nchar` are numeric, `missing_evidence` is logical, `evidence` is of list type. Lastly, `verifiable`, `sentiment_category`, and `label` (the target variable) are categorical.

Looking at the distribution of these new variables:
```{r}
# claim_nchar distribution
hist(fever$claim_nchar,breaks=50, main = paste("Histogram of claim_nchar"),col = "#b2abd2")

# missing_evidence distribution
evi_bar <- fever %>%
  ggplot(aes(x=missing_evidence, fill = missing_evidence)) + geom_bar(aes(y = (..count..)/sum(..count..))) + scale_y_continuous(labels=scales::percent) + labs(title = "Distribution of `missing_evidence`") + xlab("Missing Evidence") + ylab("Percentage") + theme_minimal()
evi_bar + scale_fill_brewer(palette = "PuOr", guide = "none")

# sentiment_category distribution
senti_bar <- fever %>%
  ggplot(aes(x=sentiment_category, fill = sentiment_category)) + geom_bar(aes(y = (..count..)/sum(..count..))) + scale_y_continuous(labels=scales::percent) + labs(title = "Distribution of `sentiment_category`") + xlab("Sentiment Category") + ylab("Percentage") + theme_minimal()
senti_bar + scale_fill_brewer(palette = "PuOr", guide = "none")

```
These visualizations show that all three of these variables are quite skewed, and these can be processed at a later stage.

Converting all of the predictors into numeric type, because the models I'm using take numeric inputs:

```{r}
fever <- fever %>%
  mutate(missing_evidence = as.numeric(missing_evidence)) %>%
  mutate(sentiment_category = case_when(sentiment_category == "Positive" ~ 1,
                           TRUE ~ 0)) %>%
  mutate(verifiable = case_when(verifiable == "VERIFIABLE" ~ 1,
                           TRUE ~ 0))
```

Also, converting `label` to factor type:

```{r}
fever$label <- as.factor(fever$label)

# renaming the factor levels so that they follow R's naming conventions
fever$label <- recode_factor(fever$label, 'NOT ENOUGH INFO' = "NOT_ENOUGH_INFO", 
                                'SUPPORTS' = "SUPPORTS",
                            'REFUTES' = "REFUTES")
```

Creating a document-feature matrix from terms in `claim`:

```{r}
corpus1 <- corpus(fever,docid_field = "id",text_field = "claim")
data_tokens <- tokens(corpus1)

# creating DFM
train_dfm <- tokens(corpus1, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url=TRUE) %>%  
  tokens_remove(stopwords("en")) %>%
  dfm()

# removing word stems from the DFM to reduce the number of features
train_dfm <- dfm_wordstem(train_dfm)

# converting the fever dataset into a matrix
train_orig <- fever %>% select(claim_nchar,missing_evidence, sentiment_category, verifiable) %>% as.matrix()

# combining the DFM with this matrix
train_matrix <- cbind(train_orig, train_dfm)

# creating a separate vector for the target variable
train_outcome <- fever %>% select(label)

```

I plan to use the 16,941 features in `train_matrix` to predict `train_outcome`.

### Train-Test Split

Randomly splitting `train_matrix` into training and testing sets in a 75:25 ratio respectively to ensure better accuracy on the test set:

```{r}
# setting the fractions of the matrix to split into training and test
training.size   <- 0.75
test.size       <- 0.25

# computing sample sizes based on proportion of data needed for each data set
training.N <- floor(training.size * nrow(train_matrix)) 
test.N <- floor(test.size * nrow(train_matrix))

# setting the seed ensures reproducibility for future analyses
set.seed(4444)
training.indices <- sort(sample(seq_len(nrow(train_matrix)), size=training.N, replace=FALSE))
test.indices <- setdiff(seq_len(nrow(train_matrix)), training.indices)

# assigning the matrices for training and test
training.data <- train_matrix[training.indices, ]
test.data <- train_matrix[test.indices, ]

# also splitting the outcome variable into matching train-test indices
train.label <- train_outcome[training.indices,]
test.label <- train_outcome[test.indices,]
```

## Evaluation Metric

This is a multi-class classification problem since there are three classes in `label`, the outcome variable. The first model I plan to use is logistic regression with LASSO (Least Absolute Shrinkage and Selection Operator) regularization. The LASSO regularization helps with feature selection, so it is optimal for working with 16,941 features. Also, it reduces overfitting, shrinks coefficients to zero, and works well with sparse data. Next, I plan to use XGBoost, which has been found to be more efficient than the random forest model. It is an ensemble method suitable for large and imbalanced datasets, since higher weight is given to the minority class at each successive iteration. Lastly, I plan to use K-Nearest Neighbors, which works well with high-dimensional data, is easily implemented in multi-class problems, is intuitive in nature, and has good predictive power.

I plan to use the Macro F1 metric to summarize the performance of my chosen models. As explained in the exploratory data analysis, the classes of the target variable `label` are imbalanced. The macro F1 score goes a step further than the F1 score, the harmonic mean of the precision and recall, in that it also takes the arithmetic mean of all of the per-class F1 scores. This gives equal weight to each category and is less likely to be biased towards the majority/minority class.

## Data Pre-processing

Since there are no missing data in this dataset, I won't be performing any data imputation. Also, I plan to use cross-validation instead of a hold-out validation set, so I won't be splitting my training set into a separate validation set. By removing the word stems from the DFM generated from the training set, I've reduced the number of possible features that can be used in the model. Since some of the predictors are skewed, I'll be scaling them when defining my models for distance-dependent measures like KNN and SVM.

## Fitting Models

I originally intended to work with K-Nearest Neighbors, logistic regression with LASSO regularization, and XGBoost for this project. However, running the models with such large-scale data meant training took a lot longer than I expected, and my system kept crashing as well. Thus, I decided to explore other methods that could handle this type of data. I chose to try the random forest and support vector machine models.
SVM handles sparse data well in general because it identified 'support vectors' in the data, which are data points that help map out as wide of an area as possible between the categories. This way, the support vectors are highlighted and all other data points and features can be ignored. The random forest algorithm works well with large datasets such as FEVER and tends to run efficiently.


### Logistic Regression with LASSO Regularization

In this model the lambda hyperparameter will be tuned. Its value can range from 0 to infinity. As lambda increases, variance decreases and bias increases. In this model, the grid defined for lambda is a logged sequence of values in increasing powers of 10 between 10^10 and 10^(-2), with 100 equally spaced values in between.

```{r}

# setting cross validation number of folds to 5
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = TRUE,
                              allowParallel = TRUE,
                            summaryFunction = multiClassSummary,
                              classProbs = TRUE)

set.seed(444)
grid <- 10^seq(10, -2, length = 100)
log_model <- train(training.data, train.label, method = "glmnet",
                             trControl = train_control,
                             tuneGrid = expand.grid(alpha = 1, lambda = grid)) 

summary(log_model)
log_model$bestTune
# alpha = 1, lambda = 0.01

# confusion matrix - train
confusionMatrix(log_model,mode = "prec_recall")

# macro F1 score - train
log_model$results
log_model$results$Mean_F1

final_control <- trainControl(method = "none",
                              verboseIter = TRUE,
                              allowParallel = TRUE,
                            summaryFunction = multiClassSummary,
                              classProbs = TRUE)
# final log model
log_final <- train(training.data, train.label, method = "glmnet",
                             trControl = final_control,
                             tuneGrid = expand.grid(alpha = 1, lambda = 0.01)) 

# fitting to test data
log_pred <- predict(log_final,test.data)

# confusion matrix - test
test_log_cm <- confusionMatrix(log_pred,test.label, mode = "prec_recall")
test_log_cm

# macro F1 score - test
macrof1_log <- mean(test_log_cm$byClass[19:21])
macrof1_log
```

### K-Nearest Neighbors

For this model, I'll be tuning the k-value (number of neighbors to be considered in classifying a datapoint). Since this model is distance-dependent, the inputs will need to be scaled.

```{r}
# scaling the training sparse matrix since KNN is a distance-based model
scaled_mat <- scale(training.data)

#checking for NA values
any(is.na(scaled_mat))

# since this returns a regular matrix, converting it back to a sparse matrix
scaled_sparse_mat <- Matrix(scaled_mat, sparse = TRUE)

# setting cross validation number of folds to 5
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = TRUE,
                              allowParallel = TRUE,
                            summaryFunction = multiClassSummary,
                              classProbs = TRUE)

knn_model <- train(scaled_mat, train.label, method = "knn", trControl=train_control, tuneGrid=expand.grid(k = c(1, 5, 7)))
summary(knn_model)

# confusion matrix - train
confusionMatrix(knn_model)

# macro F1 score - train
knn_model$results
knnFit1$results$Mean_F1

# plotting the model
plot(knn_model)

# scaling the test set by using the same mean and SD of the training set
ref_means <- colMeans(scaled_sparse_mat)
ref_sds <- apply(scaled_sparse_mat, 2, sd)
scaled_new_mat <- sweep(test.data, 2, ref_means, "-") / ref_sds
# converting it back to a sparse matrix
scaled_sparse_new_mat <- Matrix(scaled_new_mat, sparse = TRUE)

# fitting onto the test set
fin_control <- trainControl(method = "none",
                              verboseIter = TRUE,
                              allowParallel = TRUE,
                            summaryFunction = multiClassSummary,
                              classProbs = TRUE)
knn_final <- train(scaled_sparse_mat, train.label, method="knn",trControl=train_control, tuneGrid=expand.grid(k = ))
knn_pred <- predict(knn_final, scaled_sparse_new_mat)

# confusion matrix - test
test_knn_cm <- confusionMatrix(knn_pred, test.label, mode = "prec_recall")
test_knn_cm

# macro F1 score - test
macrof1_knn <- mean(test_knn_cm$byClass[19:21])
macrof1_knn

```

### XGBoost

XGBoost does not require the predictors to be scaled, so I'm not pre-processing the training data. This model has several hyperparameters to be tuned: nrounds (number of trees), max_depth (maximum depth of the tree; makes the model more complex with higher values), eta (the learning rate/shrinkage done to prevent overfitting), gamma (a regularization parameter that restricts when tree splits are made), colsample_bytree (a fraction of randomly selected features that will be used to train each tree), min_child_weight (a larger value leads to a more conservative model), subsample (proportion of rows that will be used to build trees).

```{r}
t <- trainControl(method = "cv", number = 2, verboseIter = TRUE, allowParallel = TRUE, summaryFunction = multiClassSummary,classProbs = TRUE)

grid_tune <- expand.grid(nrounds = c(500,1000,1500),
                         max_depth = c(2,4),
                         eta = 0.3,
                         gamma = 0,
                         colsample_bytree = 1,
                         min_child_weight = 1,
                         subsample = 1
                         )

xgb_model <- train(x = training.data,
                  y = train.label,
                  trControl = t,
                  tuneGrid = grid_tune,
                  method = "xgbTree",
                  verbose = TRUE)
xgb_model

# best tune
xgb_model$bestTune

# confusion matrix - train
confusionMatrix(xgb_model)

# macro F1 score - train
xgb_model$results
xgb_model$results$Mean_F1

# fitting to test data
t_control <- trainControl(method = "none",
                              verboseIter = TRUE,
                              allowParallel = TRUE)
final_grid <- expand.grid(nrounds = xgb_model$bestTune$nrounds,
                          eta = xgb_model$bestTune$eta,
                          gamma = xgb_model$bestTune$gamma,
                          colsample_bytree = xgb_model$bestTune$colsample_byTree,
                          min_child_weight = xgb_model$bestTune$min_child_weight,
                          subsample = xgb_model$bestTune$subsample)

xgb_final <- train(x = training.data,
                   y = train.label,
                   trControl = t_control,
                   tuneGrid = final_grid,
                   method = "xgbTree",
                   verbose = TRUE)
xgb_pred <- predict(xgb_final, test.data)

# confusion matrix - test
test_xgb_cm <- confusionMatrix(xgb_pred,train.label, mode = "prec_recall")

# macro f1 score - test
macrof1_xgb <- mean(test_xgb_cm$byClass[19:21])
macrof1_xgb

```

### Support Vector Machine

In this model, the hyperparameter C (regularization; reduces variance and prevents overfitting by evenly shrinking and simplifying the data) will be tuned. The larger the value of C, the smaller the margin size and lower the chance of misclassification. The grid defined for this parameter will test 10 values between 0 and 2 that are in equal intervals.

```{r}
# setting cross validation number of folds to 5
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = TRUE,
                              allowParallel = TRUE,
                            summaryFunction = multiClassSummary,
                              classProbs = TRUE)

svm_model <- train(scaled_sparse_mat, train.label, method = "svmLinear", trControl = train_control, tuneGrid = expand.grid(C = seq(0,2,length = 10)))
summary(svm_model)

svm_model$results
svm_model$bestTune

# plotting the model
plot(svm_model)

# confusion matrix - train
confusionMatrix(svm_model)

# macro F1 - train
svm_model$results
svm_model$results$Mean_F1

# fitting the final model on  test set
svm_final <- train(training.data, train.label, method = "svmLinear", trControl = t_control, tuneGrid = expand.grid(C = ))

svm_pred <- predict(svm_final, test.data)

# confusion matrix - test
test_svm_cm <- confusionMatrix(svm_pred,train.label, mode = "prec_recall")

# macro f1 score - test
macrof1_svm <- mean(test_svm_cm$byClass[19:21])
macrof1_svm

```

### Random Forest

In this model, these are the hyperparameters to be tuned:
- mtry (number of features that will be considered in the first split); default value is 4.
- min.node.size (depth of trees; number of nodes to be included in a terminal node)
- num.trees (higher number of trees, more accuracy); for this project, I considered only 100 trees because of time constraints.

```{r}
t <- trainControl(method = "cv", number = 3, verboseIter = TRUE, allowParallel = TRUE, summaryFunction = multiClassSummary,classProbs = TRUE)

tgrid <- expand.grid(mtry = 2:4,splitrule = "gini",min.node.size = c(10, 20))

rf_model <- train(na.omit(training.data),train.label, method="ranger", num.trees=100, tuneGrid = tgrid, importance = "impurity", maximize = FALSE, trControl = t)
rf_model

# Accuracy was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 2, splitrule = gini and min.node.size = 10

# confusion matrix - train
confusionMatrix(rf_model)

# macro F1 score - train
rf_model$results$Mean_F1 #NaN

# fitting the final model with values as: mtry = 2, splitrule = gini and min.node.size = 10
tgrid_final <- expand.grid(mtry = 2,splitrule = "gini",min.node.size = 10)
t_control <- trainControl(method = "none", verboseIter = TRUE,allowParallel = TRUE)

rf_final <- train(na.omit(training.data),train.label, method="ranger", num.trees=100, tuneGrid = tgrid_final, importance = "impurity", maximize = FALSE, trControl = t_control)
rf_final

# confusion matrix - test
test_svm_cm <- confusionMatrix(svm_pred,train.label, mode = "prec_recall")

# macro F1 score - test
macrof1_rf <- mean(test_rf_cm$byClass[19:21])
macrof1_rf

```

After running this model for a while, the mean F1 scores came back as NaN. Since there were no missing values in my data, this could be because the classes in the target variable are imbalanced. I wasn't able to predict its performance on the test set.

## Comparing Models

Unfortunately, I wasn't able to run all of the models I'd selected due to last-minute technical issues. I tried additional methods - SVM and Random Forest - because of their quick performance and ability to handle sparse data with many dimensions. Below, I discuss any results I was able to get and my experience building each model.

*Logistic regression with LASSO regularization*: The macro-F1 score for the training set is 0.6993266 and 0.7020029 for the test set, which are decent values since a score closer to 1 indicates that the model is able to make more accurate predictions. Also, the score was higher in the test set than the training set. The test data possibly has lesser noise than the train data, even though the split was random. This also shows that overfitting did not occur. This model uses lesser predictors, and hence, is more interpretable. This method tends to have higher bias.

*K-Nearest Neighbors*: I tried to scale my predictors before using them in my model, which generated a regular matrix that I had to convert back to a sparse matrix format. When I tried to run this model, my R session eventually crashed and I'd have to restart. I suspect that this is because of the size of the data, or because KNN does not work very well with sparse matrices. I expect this model to overfit because of the curse of dimensionality. Here, the features are sparse so even the closest neighbors/points will be quite far to provide a reasonable estimate. KNN tends to cause low bias and high variance, and increasing the value of k also increases the bias. It isn't very interpretable since the model largely depends on the k-value.

*Support Vector Machine*: like KNN, I scaled my predictors before using them in the model, and my R session kept crashing while running this model as well. This model tends to cause low bias and high variance, which can be influenced by the right 'C' regularization parameter value. While flexible, it isn't very interpretable.

*XGBoost*: This model (as of the time of submission) has been running for hours, so I haven't been able to view and interpret the results of this model. I expect it to perform the best because it is an ensemble method that combines other ML methods. Also, it is at lower risk of overfitting the data because it used regularization parameters such as `min.child.weight`. In terms of bias and variance, it takes a weighted average of many weak models and iterates through them, so both tend to be effectively reduced. However, it isn't very interpretable and will need other methods to comprehend it better.

*Random Forest*: Running this model returned NaN values for the mean F1 scores, so I couldn't interpret and compare the scores on the test and train sets. Like XGBoost, another tree-based method, it is at risk of overfitting to the data. Also, since the data are imbalanced, I expect the trees formed to be biased towards the majority class. While it works well to reduce variance, it could amplify bias due to the imbalanced nature of the data. Also, since it involves combining many decision trees, it can be less interpretable. It likely will produce low bias because it works with random subsets of features, and has lower variance than individual decision trees.


## Ethical Implications

The FEVER dataset is publicly available, well-researched and is transparent in its methodology. Its creators regularly hold the FEVER competition, in which people are encouraged to build efficient machine learning models using this dataset. Till date, the best models have a FEVER score of around 70% (Christodoulopoulos & Mittal, 2019). In this dataset, there may be concerns of generalizability to the real-world context. For instance, the claims were extracted from Wikipedia and altered artificially by annotators. Also, as viewed in the exploratory data analysis, punctuation marks were consistent and were not out-of-place. However, in real life data, especially on social media, there could be unexpected symbols or misspelt words in text that the model has not really been trained on. It does not consider other methods of assessing credibility, such as stylistic, tonal, and the rate at which the information is propagated online.

Another concern related to robustness of the model is that, from the textual analysis of the `claim` column, we understand that there are many claims related to the Hollywood film industry. Thus, the dataset is quite biased and there aren't a diverse range of topics/words that the model has been exposed to, which could potentially lead to poor performance on unseen data. In the models I built, the `evidence` column was not utilized, so the claims were not contextualized. This could be a cause for concern because factual credibility of the claims was not established. Additionally, while KNN is fairly explainable, the XGBoost model has limited explainability, raising transparency concerns. For these reasons, it is evident that fact verification still requires some form of human judgement and context sensitivity and cannot yet be completely automated by machine learning models.

## Conclusion, Limitations, and Future Directions

Fact verification is extremely important in today's day and age, considering how quickly misinformation can spread. Automating the process of fact-checking helps deal with the barrage of information we are exposed to on a daily basis. Thus, this project aimed to use machine learning and natural language processing methods to classify altered Wikipedia claims from the FEVER dataset. While I wasn't able to run all of my selected models because of time constraints, I was able to view the results for the logisitic regression with LASSO regularization. It performed quite well with test and train macro F1 scores around 0.6-0.7.

A significant challenge I faced in this project was working with sparse data, as I needed to find models that work well on it. Since it had many features, the run-time of each model was high. Additionally, I constantly ran into issues with exhausting the vector memory, and had to manually increase the memory allocated in my R environment. My R session kept aborting while running KNN and SVM, possibly because of the data size. Also, from the model results, I took the mean of the F1 scores of the individual classes and compared that across models. I could not figure out how to implement a custom macro-F1 function in caret despite its documentation mentioning that custom evaluation metrics could be utilized.

Future projects could apply some dimensionality reduction techniques that focus only on the most important features. The hyperparameters of each of the models selected could be tuned better, and additional evaluation metrics that are weighted according to the class distribution in the target variable could be used for comparison. One way in which this project can be improved is by using the `evidence` variable, which detects and extracts information from Wikipedia to actually verify the claims. Also, other classification models could be used to compare relative performances. It would be interesting to see how pre-trained language models like BERT function with this dataset. Fact verification should be explored and automated in other languages as well. Since a lot of datasets like this are usually completely in English, models trained on them will work better on English text compared to other text types.


## References

Attal, M. (2020, September 9). Fact checked! are automated fact checkers the solution to fake news? (Part 1). Medium. https://medium.com/humanid/fact-checked-are-automated-fact-checkers-the-solution-to-fake-news-part-1-7a2760483e3c.

Christodoulopoulos, C., & Mittal, A. (2019, October 17). The FEVER data set: What doesn't kill it will make it stronger. Amazon Science. https://www.amazon.science/blog/the-fever-data-set-what-doesnt-kill-it-will-make-it-stronger.

El Houfi, O. (2022, June 6). Using Language Models for Fact-Checking & Claim Assessment. Weights & Biases. https://wandb.ai/othmanelhoufi/LM-for-fact-checking/reports/Using-Language-Models-for-Fact-Checking-Claim-Assessment--VmlldzoyMTIzNzA2?galleryTag=transformer.

Fact Verification. (n.d.). Papers With Code. https://paperswithcode.com/task/fact-verification.

Taboada, M. (2021, September 7). Authentic language in fake news. Items. https://items.ssrc.org/beyond-disinformation/authentic-language-in-fake-news/.

Thorne, J., Vlachos, A., Christodoulopoulos, C., & Mittal, A. (2018). FEVER: a large-scale dataset for Fact Extraction and VERification. Computation and Language. https://doi.org/https://doi.org/10.48550/arXiv.1803.05355.
